{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install openai langchain redis tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LLM Models\n",
    "\n",
    "- Import libraries\n",
    "- Configure access information and paths\n",
    "- Configure model parameters\n",
    "- Set Redis connection\n",
    "- Configure Azure Cache for Redis to be used as a semantic cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import redis\n",
    "import os\n",
    "import langchain\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import RedisSemanticCache\n",
    "import time\n",
    "\n",
    "\n",
    "AZURE_ENDPOINT=\"<azure-openai-endpoint>\"\n",
    "API_KEY=\"<azure-openai-key>\"\n",
    "API_VERSION=\"2023-05-15\"\n",
    "LLM_DEPLOYMENT_NAME=\"<name-of-your-gpt-deployment>\"\n",
    "LLM_MODEL_NAME=\"gpt-35-turbo-instruct\"\n",
    "EMBEDDINGS_DEPLOYMENT_NAME=\"<name-of-your-embeddings-deployment>\"\n",
    "EMBEDDINGS_MODEL_NAME=\"text-embedding-ada-002\"\n",
    "\n",
    "REDIS_ENDPOINT = \"<azure-cache-for-redis-endpoint-name>\" # must include port at the end. e.g. \"redisdemo.eastus.redisenterprise.cache.azure.net:10000\"\n",
    "REDIS_PASSWORD = \"<azure-cache-for-redis-password>\"\n",
    "\n",
    "# make sure you have an LLM deployed in your Azure Open AI account. In this example, I used the GPT 3.5 turbo instruct model. My deployment was named \"gpt35instruct\".\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=LLM_DEPLOYMENT_NAME,\n",
    "    model_name=\"gpt-35-turbo-instruct\",\n",
    "    openai_api_key=API_KEY,\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    openai_api_version=API_VERSION,\n",
    ")\n",
    "# make sure you have an embeddings model deployed in your Azure Open AI account. In this example, I used the text embedding ada 002 model. My deployment was named \"textembedding\".\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=API_KEY,\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    openai_api_version=API_VERSION\n",
    ")\n",
    "\n",
    "# create a connection string for the Redis Vector Store. Uses Redis-py format: https://redis-py.readthedocs.io/en/stable/connections.html#redis.Redis.from_url\n",
    "# This example assumes TLS is enabled. If not, use \"redis://\" instead of \"rediss://\n",
    "redis_url = \"rediss://:\" + REDIS_PASSWORD + \"@\"+ REDIS_ENDPOINT\n",
    "\n",
    "# set up the semantic cache for your llm\n",
    "set_llm_cache(RedisSemanticCache(redis_url = redis_url, embedding=embeddings, score_threshold=0.01))\n",
    "\n",
    "#note: you can use score_threshold to change how sensitive the semantic cache is. The lower the score, the less likely it is to use a cached result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the LLM\n",
    "Try runnning again with different queries to see what is cached and what is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Oh, little balls of fluff and fur\n",
      "With wagging tails and tiny paws\n",
      "Puppies, oh puppies, so pure\n",
      "The epitome of cuteness, no flaws\n",
      "\n",
      "With big round eyes that melt our hearts\n",
      "And floppy ears that bounce with glee\n",
      "Their playful antics, like works of art\n",
      "They bring joy to all they see\n",
      "\n",
      "Their soft, warm bodies, so cuddly\n",
      "As they curl up in our laps\n",
      "Their gentle kisses, so lovingly\n",
      "Like tiny, wet, puppy taps\n",
      "\n",
      "Their clumsy steps and wobbly walks\n",
      "As they explore the world anew\n",
      "Their curiosity, like a ticking clock\n",
      "Always eager to learn and pursue\n",
      "\n",
      "Their little barks and yips so sweet\n",
      "Fill our days with endless delight\n",
      "Their unconditional love, so complete\n",
      "Makes everything feel just right\n",
      "\n",
      "Oh, how can one resist their charms\n",
      "As they snuggle close and nuzzle our face\n",
      "With their innocent eyes and wagging arms\n",
      "They fill our hearts with endless grace\n",
      "\n",
      "Puppies, oh puppies, so precious and dear\n",
      "Forever in our hearts, they'll stay\n",
      "For they bring us love and laughter, year after year\n",
      "Our cute little pups, in every way.\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = llm(\"Write a poem about cute puppies.\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
