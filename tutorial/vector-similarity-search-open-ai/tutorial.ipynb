{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "Install the python dependencies required for our application. Using a Python virtual environment is usually a good idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set up Azure OpenAI service connection\n",
    "Fill in the Azure OpenAI information below to establish the connection to the Azure OpenAI model. This example stores these values in application variables for the sake of simplicity. Outside of tutorials, it's strongly recommended to store these in environment variables or using a secrets manager like Azure KeyVault. \n",
    "\n",
    "Note that there are differences  between the `OpenAI` and `Azure OpenAI` endpoints. This example uses the configuration for `Azure OpenAI`. See [How to switch between OpenAI and Azure OpenAI endpoints with Python](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding\n",
    "import tiktoken\n",
    "from typing import List\n",
    "\n",
    "API_KEY = \"<azure-openai-key>\"\n",
    "RESOURCE_ENDPOINT = \"<azure-openai-endpoint>\" # e.g. https://openaiexample.openai.azure.com/\n",
    "DEPLOYMENT_NAME = \"my-embedding-model\" # this is the name you selected for your deployment, not the name of the OpenAI model. \n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "url = openai.api_base + \"openai/models/\" + DEPLOYMENT_NAME + \"?api-version=2023-05-15\" \n",
    "\n",
    "r = requests.get(url, headers={\"api-key\": API_KEY})\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset\n",
    "\n",
    "This example uses the [Wikipedia Movie Plots](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots) dataset from Kaggle. Download this file and place it in the same directory as this jupyter notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(os.path.join(os.getcwd(),'wiki_movie_plots_deduped.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the dataset to remove spaces in the column titles and filter the dataset to lower the size. This isn't required, but is helpful in reducing the time it takes to generate embeddings and loading the index into Redis. Feel free to play around with the filters, or add your own! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, 'id', range(0, len(df)))\n",
    "df['year'] = df['Release Year'].astype(int)\n",
    "df['origin'] = df['Origin/Ethnicity'].astype(str)\n",
    "del df['Release Year']\n",
    "del df['Origin/Ethnicity']\n",
    "df = df[df.year > 1970] # only movies made after 1970\n",
    "df = df[df.origin.isin(['American','British','Canadian'])] # only movies from English-speaking cinema\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove whitespace from the `Plot` column to make it easier to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# s is input text\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "df['Plot']= df['Plot'].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of tokens required to generate the embeddings for this dataset. You may want to filter the dataset more stringently in order to limit the tokens required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "df['n_tokens'] = df[\"Plot\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df = df[df.n_tokens<8192]\n",
    "print('Number of movies: ' + str(len(df))) # print number of movies remaining in dataset\n",
    "print('Number of tokens required:' + str(df['n_tokens'].sum())) # print number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings\n",
    "\n",
    "This function calls Azure OpenAI service to generate the embeddigns and add them to the dataframe in a column entitled `embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df[\"Plot\"].apply(lambda x : get_embedding(x, engine = DEPLOYMENT_NAME))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Redis\n",
    "This example uses [Azure Cache for Redis](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-overview), a fully managed Redis service on Azure. The [Enterprise tier](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/quickstart-create-redis-enterprise) of Azure Cache for Redis features the [RediSearch](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-redis-modules#redisearch) module, which includes vector search capability. This example stores these values in application variables for the sake of simplicity. Outside of tutorials, it's strongly recommended to store these in environment variables or using a secrets manager like Azure KeyVault.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "from redis.commands.search.indexDefinition import (\n",
    "    IndexDefinition,\n",
    "    IndexType\n",
    ")\n",
    "from redis.commands.search.query import Query\n",
    "from redis.commands.search.field import (\n",
    "    TextField,\n",
    "    VectorField\n",
    ")\n",
    "\n",
    "REDIS_HOST =  \"<redis-endpoint>\" # e.g. redisdemo.southcentralus.redisenterprise.cache.azure.net. If you're copying and pasting from the Azure portal, you do not need the \":10000\" at the end of the hostname. \n",
    "REDIS_PORT = 10000  # default for Azure Cache for Redis Enterprise\n",
    "REDIS_PASSWORD = \"<redis-access-key>\"\n",
    "\n",
    "# Connect to Redis\n",
    "redis_client = redis.Redis(\n",
    "    host=REDIS_HOST,\n",
    "    port=REDIS_PORT,\n",
    "    password=REDIS_PASSWORD\n",
    ")\n",
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fields for the vector index of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "VECTOR_DIM = 1536                               # length of the vectors for the text-embedding-ada-002 (Version 2) model\n",
    "VECTOR_NUMBER = len(df)                         # initial number of vectors\n",
    "INDEX_NAME = \"embeddings-index\"                 # name of the search index\n",
    "PREFIX = \"movie\"                                # prefix for the document keys\n",
    "DISTANCE_METRIC = \"COSINE\"                      # distance metric for the vectors (ex. COSINE, IP, L2)\n",
    "\n",
    "# Define RediSearch fields for each of the columns in the dataset\n",
    "title = TextField(name=\"Title\")\n",
    "director = TextField(name=\"Director\")\n",
    "cast = TextField(name=\"Cast\")\n",
    "genre = TextField(name=\"Genre\")\n",
    "embeddings_vectors = VectorField(\"embeddings\",\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": VECTOR_DIM,\n",
    "        \"DISTANCE_METRIC\": DISTANCE_METRIC,\n",
    "        \"INITIAL_CAP\": VECTOR_NUMBER,\n",
    "    }\n",
    ")\n",
    "fields = [title, director, cast, genre, embeddings_vectors]\n",
    "\n",
    "# Check if index exists\n",
    "try:\n",
    "   redis_client.ft(INDEX_NAME).info()\n",
    "   # print(redis_client.ft(INDEX_NAME).info())\n",
    "   # print(\"Index already exists\")\n",
    "except:\n",
    "    # Create RediSearch Index\n",
    "    redis_client.ft(INDEX_NAME).create_index(\n",
    "        fields = fields,\n",
    "        definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the index documents into Redis. This can take 10+ minutes, depending on the dataset size used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(client: redis.Redis, prefix: str, documents: pd.DataFrame):\n",
    "    records = documents.to_dict(\"records\")\n",
    "    for doc in records:\n",
    "        key = f\"{prefix}:{str(doc['id'])}\"\n",
    "\n",
    "        # create byte vectors for title and content\n",
    "        plot_embeddings = np.array(doc[\"embeddings\"], dtype=np.float32).tobytes()\n",
    "\n",
    "        # replace list of floats with byte vectors\n",
    "        doc[\"embeddings\"] = plot_embeddings\n",
    "\n",
    "        client.hset(key, mapping = doc)\n",
    "\n",
    "index_documents(redis_client, PREFIX, df)\n",
    "print(f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run search queries\n",
    "\n",
    "First, we setup the search parameters, defining the index and vector fields we're using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_redis(\n",
    "    redis_client: redis.Redis,\n",
    "    user_query: str,\n",
    "    index_name: str = \"embeddings-index\",\n",
    "    vector_field: str = \"embeddings\",\n",
    "    return_fields: list = [\"Title\", \"Director\", \"Cast\", \"Genre\", \"vector_score\"],\n",
    "    hybrid_fields = \"*\",\n",
    "    k: int = 20,\n",
    ") -> List[dict]:\n",
    "\n",
    "    # Creates embedding vector from user query\n",
    "    embedded_query = openai.Embedding.create(deployment_id=DEPLOYMENT_NAME, input=user_query,\n",
    "                                            model=DEPLOYMENT_NAME,\n",
    "                                            )[\"data\"][0]['embedding']\n",
    "\n",
    "    # Prepare the Query\n",
    "    base_query = f'{hybrid_fields}=>[KNN {k} @{vector_field} $vector AS vector_score]'\n",
    "    query = (\n",
    "        Query(base_query)\n",
    "         .return_fields(*return_fields)\n",
    "         .sort_by(\"vector_score\")\n",
    "         .paging(0, k)\n",
    "         .dialect(2)\n",
    "    )\n",
    "    params_dict = {\"vector\": np.array(embedded_query).astype(dtype=np.float32).tobytes()}\n",
    "\n",
    "    # perform vector search\n",
    "    results = redis_client.ft(index_name).search(query, params_dict)\n",
    "    for i, article in enumerate(results.docs):\n",
    "        score = 1 - float(article.vector_score)\n",
    "        print(f\"{i}.{article.Title} (Score: {round(score ,3) })\")\n",
    "    return results.docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the function we just defined to search for any plain-text query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_redis(redis_client, \"A movie with old airplanes in it \", k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hybrid queries\n",
    "\n",
    "You can also run hybrid queries, that is, queries that use both vector search and filters based on other parameters in the dataset. In this case, we filter our query results to only movies tagged with the `drama` genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_field(field_name: str, value: str) -> str:\n",
    "    return f'@{field_name}:\"{value}\"'\n",
    "\n",
    "# search the content vector for movies with a specific genre\n",
    "results = search_redis(redis_client,\n",
    "                       \"Basketball comeback story with NBA players\",\n",
    "                       vector_field=\"embeddings\",\n",
    "                       k=10,\n",
    "                       hybrid_fields=create_hybrid_field(\"Genre\", \"drama\")\n",
    "                       )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

